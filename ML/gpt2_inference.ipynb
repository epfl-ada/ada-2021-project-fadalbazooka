{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-12-14T14:58:29.745044Z",
     "iopub.status.busy": "2021-12-14T14:58:29.744719Z",
     "iopub.status.idle": "2021-12-14T14:59:28.871627Z",
     "shell.execute_reply": "2021-12-14T14:59:28.870440Z",
     "shell.execute_reply.started": "2021-12-14T14:58:29.744958Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install transformers library.\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git\n",
    "# Install helper functions.\n",
    "!pip install -q git+https://github.com/gmihaila/ml_things.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T14:59:28.876129Z",
     "iopub.status.busy": "2021-12-14T14:59:28.875731Z",
     "iopub.status.idle": "2021-12-14T14:59:37.839051Z",
     "shell.execute_reply": "2021-12-14T14:59:37.838014Z",
     "shell.execute_reply.started": "2021-12-14T14:59:28.876054Z"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np \n",
    "import os\n",
    "import pandas as pd \n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ml_things import plot_dict, plot_confusion_matrix, fix_text\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import (set_seed,\n",
    "                          TrainingArguments,\n",
    "                          Trainer,\n",
    "                          GPT2Config,\n",
    "                          GPT2Tokenizer,\n",
    "                          AdamW, \n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          GPT2ForSequenceClassification)\n",
    "\n",
    "# Set seed for reproducibility.\n",
    "set_seed(123)\n",
    "\n",
    "# Number of training epochs (authors on fine-tuning Bert recommend between 2 and 4).\n",
    "epochs = 5\n",
    "\n",
    "# Number of batches - depending on the max sequence length and GPU memory.\n",
    "# For 512 sequence length batch of 10 works without cuda memory issues.\n",
    "# For small sequence length can try batch of 32 or higher.\n",
    "batch_size = 8\n",
    "\n",
    "# Pad or truncate text sequences to a specific length\n",
    "# if `None` it will use maximum sequence of word piece tokens allowed by model.\n",
    "max_length = None\n",
    "\n",
    "# Look for gpu to use. Will use `cpu` by default if no gpu found.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Name of transformers model - will use already pretrained model.\n",
    "# Path of transformer model - will load your own model from local disk.\n",
    "model_name_or_path = 'gpt2'\n",
    "\n",
    "# Dictionary of labels and their id - this will be used to convert.\n",
    "# String labels to number ids.\n",
    "labels_ids = {'neg': 0, 'pos': 1}\n",
    "\n",
    "# How many labels are we using in training.\n",
    "# This is used to decide size of classification head.\n",
    "n_labels = len(labels_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T14:59:37.843482Z",
     "iopub.status.busy": "2021-12-14T14:59:37.842769Z",
     "iopub.status.idle": "2021-12-14T14:59:37.854963Z",
     "shell.execute_reply": "2021-12-14T14:59:37.853985Z",
     "shell.execute_reply.started": "2021-12-14T14:59:37.843437Z"
    }
   },
   "outputs": [],
   "source": [
    "class QuoteDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset which loads quotes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mode=\"train\", path=\"../input/nowiki/nowikidata_clean.json\"):\n",
    "        \"\"\"\n",
    "        Initialize dataset parameters\n",
    "        :param path: path to json dataset\n",
    "        \"\"\"\n",
    "        self.no_label_data = pd.read_json(path)  # get full data \n",
    "        self.data = pd.read_json(\"../input/quotes-gender/filtered.json\")\n",
    "        countries = pd.Series([elem[0] if elem[0] is not None else \"F\" for elem in self.data.nationality])  # filter in case of NaN\n",
    "        self.data.nationality = countries.values  # filter NaN\n",
    "        self.data = self.data[self.data.nationality != \"F\"]\n",
    "        \n",
    "        # class balance and remove rare countries \n",
    "        dict_countries = self.data.nationality.value_counts()  # how many times each nationality occurs   \n",
    "        countries = dict_countries[dict_countries > 1000].keys()  # find frequent nationalities \n",
    "        self.len_nationalities = len(countries)\n",
    "        self.dict_nationalities = {elem: idx for idx, elem in enumerate(countries)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.no_label_data.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        raw_quote = self.no_label_data.Quote.iloc[item]\n",
    "        raw_nationality = \"Ireland\"\n",
    "        return {'text': raw_quote,\n",
    "                'label': raw_nationality}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T14:59:37.858510Z",
     "iopub.status.busy": "2021-12-14T14:59:37.857739Z",
     "iopub.status.idle": "2021-12-14T14:59:37.914771Z",
     "shell.execute_reply": "2021-12-14T14:59:37.913599Z",
     "shell.execute_reply.started": "2021-12-14T14:59:37.858468Z"
    }
   },
   "outputs": [],
   "source": [
    "class Gpt2ClassificationCollator(object):\n",
    "    r\"\"\"\n",
    "    Data Collator used for GPT2 in a classificaiton rask. \n",
    "    \n",
    "    It uses a given tokenizer and label encoder to convert any text and labels to numbers that \n",
    "    can go straight into a GPT2 model.\n",
    "\n",
    "    This class is built with reusability in mind: it can be used as is as long\n",
    "    as the `dataloader` outputs a batch in dictionary format that can be passed \n",
    "    straight into the model - `model(**batch)`.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "      use_tokenizer (:obj:`transformers.tokenization_?`):\n",
    "          Transformer type tokenizer used to process raw text into numbers.\n",
    "\n",
    "      labels_ids (:obj:`dict`):\n",
    "          Dictionary to encode any labels names into numbers. Keys map to \n",
    "          labels names and Values map to number associated to those labels.\n",
    "\n",
    "      max_sequence_len (:obj:`int`, `optional`)\n",
    "          Value to indicate the maximum desired sequence to truncate or pad text\n",
    "          sequences. If no value is passed it will used maximum sequence size\n",
    "          supported by the tokenizer and model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, use_tokenizer, labels_encoder, max_sequence_len=None):\n",
    "\n",
    "        # Tokenizer to be used inside the class.\n",
    "        self.use_tokenizer = use_tokenizer\n",
    "        # Check max sequence length.\n",
    "        self.max_sequence_len = use_tokenizer.model_max_length if max_sequence_len is None else max_sequence_len\n",
    "        # Label encoder used inside the class.\n",
    "        self.labels_encoder = labels_encoder\n",
    "\n",
    "        return\n",
    "\n",
    "    def __call__(self, sequences):\n",
    "        r\"\"\"\n",
    "        This function allowes the class objesct to be used as a function call.\n",
    "        Sine the PyTorch DataLoader needs a collator function, I can use this \n",
    "        class as a function.\n",
    "\n",
    "        Arguments:\n",
    "\n",
    "          item (:obj:`list`):\n",
    "              List of texts and labels.\n",
    "\n",
    "        Returns:\n",
    "          :obj:`Dict[str, object]`: Dictionary of inputs that feed into the model.\n",
    "          It holddes the statement `model(**Returned Dictionary)`.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get all texts from sequences list.\n",
    "        texts = [sequence['text'] for sequence in sequences]\n",
    "        # Get all labels from sequences list.\n",
    "        labels = [sequence['label'] for sequence in sequences]\n",
    "        # Encode all labels using label encoder.\n",
    "        labels = [self.labels_encoder[label] for label in labels]\n",
    "        # Call tokenizer on all texts to convert into tensors of numbers with \n",
    "        # appropriate padding.\n",
    "        inputs = self.use_tokenizer(text=texts, return_tensors=\"pt\", padding=True, truncation=True,  max_length=self.max_sequence_len)\n",
    "        # Update the inputs with the associated encoded labels as tensor.\n",
    "        inputs.update({'labels':torch.tensor(labels)})\n",
    "\n",
    "        return inputs\n",
    "\n",
    "\n",
    "def train(dataloader, optimizer_, scheduler_, device_):\n",
    "  r\"\"\"\n",
    "  Train pytorch model on a single pass through the data loader.\n",
    "\n",
    "  It will use the global variable `model` which is the transformer model \n",
    "  loaded on `_device` that we want to train on.\n",
    "\n",
    "  This function is built with reusability in mind: it can be used as is as long\n",
    "    as the `dataloader` outputs a batch in dictionary format that can be passed \n",
    "    straight into the model - `model(**batch)`.\n",
    "\n",
    "  Arguments:\n",
    "\n",
    "      dataloader (:obj:`torch.utils.data.dataloader.DataLoader`):\n",
    "          Parsed data into batches of tensors.\n",
    "\n",
    "      optimizer_ (:obj:`transformers.optimization.AdamW`):\n",
    "          Optimizer used for training.\n",
    "\n",
    "      scheduler_ (:obj:`torch.optim.lr_scheduler.LambdaLR`):\n",
    "          PyTorch scheduler.\n",
    "\n",
    "      device_ (:obj:`torch.device`):\n",
    "          Device used to load tensors before feeding to model.\n",
    "\n",
    "  Returns:\n",
    "\n",
    "      :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted\n",
    "        Labels, Train Average Loss].\n",
    "  \"\"\"\n",
    "\n",
    "  # Use global variable for model.\n",
    "  global model\n",
    "\n",
    "  # Tracking variables.\n",
    "  predictions_labels = []\n",
    "  true_labels = []\n",
    "  # Total loss for this epoch.\n",
    "  total_loss = 0\n",
    "\n",
    "  # Put the model into training mode.\n",
    "  model.train()\n",
    "\n",
    "  # For each batch of training data...\n",
    "  for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "\n",
    "    # Add original labels - use later for evaluation.\n",
    "    true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "    \n",
    "    # move batch to device\n",
    "    batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
    "    \n",
    "    # Always clear any previously calculated gradients before performing a\n",
    "    # backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Perform a forward pass (evaluate the model on this training batch).\n",
    "    # This will return the loss (rather than the model output) because we\n",
    "    # have provided the `labels`.\n",
    "    # The documentation for this a bert model function is here: \n",
    "    # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "    outputs = model(**batch)\n",
    "\n",
    "    # The call to `model` always returns a tuple, so we need to pull the \n",
    "    # loss value out of the tuple along with the logits. We will use logits\n",
    "    # later to calculate training accuracy.\n",
    "    loss, logits = outputs[:2]\n",
    "\n",
    "    # Accumulate the training loss over all of the batches so that we can\n",
    "    # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "    # single value; the `.item()` function just returns the Python value \n",
    "    # from the tensor.\n",
    "    total_loss += loss.item()\n",
    "\n",
    "    # Perform a backward pass to calculate the gradients.\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip the norm of the gradients to 1.0.\n",
    "    # This is to help prevent the \"exploding gradients\" problem.\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # Update parameters and take a step using the computed gradient.\n",
    "    # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "    # modified based on their gradients, the learning rate, etc.\n",
    "    optimizer_.step()\n",
    "\n",
    "    # Update the learning rate.\n",
    "    scheduler_.step()\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "\n",
    "    # Convert these logits to list of predicted labels values.\n",
    "    predictions_labels += logits.argmax(axis=-1).flatten().tolist()\n",
    "\n",
    "  # Calculate the average loss over the training data.\n",
    "  avg_epoch_loss = total_loss / len(dataloader)\n",
    "  \n",
    "  # Return all true labels and prediction for future evaluations.\n",
    "  return true_labels, predictions_labels, avg_epoch_loss\n",
    "\n",
    "\n",
    "\n",
    "def validation(dataloader, device_):\n",
    "  r\"\"\"Validation function to evaluate model performance on a \n",
    "  separate set of data.\n",
    "\n",
    "  This function will return the true and predicted labels so we can use later\n",
    "  to evaluate the model's performance.\n",
    "\n",
    "  This function is built with reusability in mind: it can be used as is as long\n",
    "    as the `dataloader` outputs a batch in dictionary format that can be passed \n",
    "    straight into the model - `model(**batch)`.\n",
    "\n",
    "  Arguments:\n",
    "\n",
    "    dataloader (:obj:`torch.utils.data.dataloader.DataLoader`):\n",
    "          Parsed data into batches of tensors.\n",
    "\n",
    "    device_ (:obj:`torch.device`):\n",
    "          Device used to load tensors before feeding to model.\n",
    "\n",
    "  Returns:\n",
    "    \n",
    "    :obj:`List[List[int], List[int], float]`: List of [True Labels, Predicted\n",
    "        Labels, Train Average Loss]\n",
    "  \"\"\"\n",
    "\n",
    "  # Use global variable for model.\n",
    "  global model\n",
    "\n",
    "  # Tracking variables\n",
    "  predictions_labels = []\n",
    "  true_labels = []\n",
    "  #total loss for this epoch.\n",
    "  total_loss = 0\n",
    "  results = []  \n",
    "\n",
    "  # Put the model in evaluation mode--the dropout layers behave differently\n",
    "  # during evaluation.\n",
    "  model.eval()\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "\n",
    "    # add original labels\n",
    "    true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "\n",
    "    # move batch to device\n",
    "    batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and\n",
    "    # speeding up validation\n",
    "    with torch.no_grad():        \n",
    "\n",
    "        # Forward pass, calculate logit predictions.\n",
    "        # This will return the logits rather than the loss because we have\n",
    "        # not provided labels.\n",
    "        # token_type_ids is the same as the \"segment ids\", which \n",
    "        # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "        # The documentation for this `model` function is here: \n",
    "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple along with the logits. We will use logits\n",
    "        # later to to calculate training accuracy.\n",
    "        loss, logits = outputs[:2]\n",
    "        \n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        # Accumulate the training loss over all of the batches so that we can\n",
    "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "        # single value; the `.item()` function just returns the Python value \n",
    "        # from the tensor.\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # get predicitons to list\n",
    "        predict_content = logits.argmax(axis=-1).flatten().tolist()\n",
    "\n",
    "        # update list\n",
    "        predictions_labels += predict_content\n",
    "\n",
    "  # Calculate the average loss over the training data.\n",
    "  avg_epoch_loss = total_loss / len(dataloader)\n",
    "\n",
    "  # Return all true labels and prediciton for future evaluations.\n",
    "  return true_labels, predictions_labels, avg_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T14:59:37.916705Z",
     "iopub.status.busy": "2021-12-14T14:59:37.916348Z",
     "iopub.status.idle": "2021-12-14T14:59:45.315384Z",
     "shell.execute_reply": "2021-12-14T14:59:45.314422Z",
     "shell.execute_reply.started": "2021-12-14T14:59:37.916639Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get model's tokenizer.\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path=model_name_or_path)\n",
    "# default to left padding\n",
    "tokenizer.padding_side = \"left\"\n",
    "# Define PAD Token = EOS Token = 50256\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T14:59:45.317567Z",
     "iopub.status.busy": "2021-12-14T14:59:45.317190Z",
     "iopub.status.idle": "2021-12-14T14:59:47.980288Z",
     "shell.execute_reply": "2021-12-14T14:59:47.978555Z",
     "shell.execute_reply.started": "2021-12-14T14:59:45.317520Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Dealing with Validation...')\n",
    "# Create pytorch dataset.\n",
    "valid_dataset =  QuoteDataset(\"val\")\n",
    "print('Created `valid_dataset` with %d examples!'%len(valid_dataset))\n",
    "\n",
    "labels_ids = valid_dataset.dict_nationalities\n",
    "n_labels = len(labels_ids)\n",
    "gpt2_classificaiton_collator = Gpt2ClassificationCollator(use_tokenizer=tokenizer, \n",
    "                                                          labels_encoder=labels_ids, \n",
    "                                                          max_sequence_len=max_length)\n",
    "# Move pytorch dataset into dataloader.\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=gpt2_classificaiton_collator)\n",
    "print('Created `eval_dataloader` with %d batches!'%len(valid_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T14:59:47.982135Z",
     "iopub.status.busy": "2021-12-14T14:59:47.981880Z",
     "iopub.status.idle": "2021-12-14T14:59:47.992153Z",
     "shell.execute_reply": "2021-12-14T14:59:47.991120Z",
     "shell.execute_reply.started": "2021-12-14T14:59:47.982097Z"
    }
   },
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T14:59:47.994716Z",
     "iopub.status.busy": "2021-12-14T14:59:47.993933Z",
     "iopub.status.idle": "2021-12-14T15:00:11.078582Z",
     "shell.execute_reply": "2021-12-14T15:00:11.077505Z",
     "shell.execute_reply.started": "2021-12-14T14:59:47.994669Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get model configuration.\n",
    "print('Loading configuraiton...')\n",
    "model_config = GPT2Config.from_pretrained(pretrained_model_name_or_path=model_name_or_path, num_labels=n_labels)\n",
    "\n",
    "\n",
    "# Get the actual model.\n",
    "print('Loading model...')\n",
    "model = GPT2ForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path, config=model_config)\n",
    "\n",
    "# resize model embedding to match new tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# fix model padding token id\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T15:02:59.289272Z",
     "iopub.status.busy": "2021-12-14T15:02:59.288500Z",
     "iopub.status.idle": "2021-12-14T15:02:59.789561Z",
     "shell.execute_reply": "2021-12-14T15:02:59.788371Z",
     "shell.execute_reply.started": "2021-12-14T15:02:59.289234Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"../input/weights/checkpoint.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# Load model to defined device.\n",
    "model.to(device)\n",
    "print('Model loaded to `%s`'%device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T15:03:02.981973Z",
     "iopub.status.busy": "2021-12-14T15:03:02.980901Z",
     "iopub.status.idle": "2021-12-14T15:06:34.754853Z",
     "shell.execute_reply": "2021-12-14T15:06:34.753716Z",
     "shell.execute_reply.started": "2021-12-14T15:03:02.981924Z"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "valid_labels, valid_predict, val_loss = validation(valid_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T15:14:15.678536Z",
     "iopub.status.busy": "2021-12-14T15:14:15.678241Z",
     "iopub.status.idle": "2021-12-14T15:14:15.686434Z",
     "shell.execute_reply": "2021-12-14T15:14:15.684978Z",
     "shell.execute_reply.started": "2021-12-14T15:14:15.678501Z"
    }
   },
   "outputs": [],
   "source": [
    "decoder = {elem:idx for idx, elem in valid_dataset.dict_nationalities.items()}\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T15:14:00.258899Z",
     "iopub.status.busy": "2021-12-14T15:14:00.258575Z",
     "iopub.status.idle": "2021-12-14T15:14:00.267736Z",
     "shell.execute_reply": "2021-12-14T15:14:00.266617Z",
     "shell.execute_reply.started": "2021-12-14T15:14:00.258866Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = [decoder[i] for i in valid_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T15:14:15.701210Z",
     "iopub.status.busy": "2021-12-14T15:14:15.700855Z",
     "iopub.status.idle": "2021-12-14T15:14:15.717041Z",
     "shell.execute_reply": "2021-12-14T15:14:15.715991Z",
     "shell.execute_reply.started": "2021-12-14T15:14:15.701177Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_dataset.no_label_data[\"nationality\"] = pd.Series(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T15:15:07.304593Z",
     "iopub.status.busy": "2021-12-14T15:15:07.304279Z",
     "iopub.status.idle": "2021-12-14T15:15:07.320506Z",
     "shell.execute_reply": "2021-12-14T15:15:07.319399Z",
     "shell.execute_reply.started": "2021-12-14T15:15:07.304563Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_dataset.no_label_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-14T15:16:16.048133Z",
     "iopub.status.busy": "2021-12-14T15:16:16.047831Z",
     "iopub.status.idle": "2021-12-14T15:16:16.150753Z",
     "shell.execute_reply": "2021-12-14T15:16:16.149637Z",
     "shell.execute_reply.started": "2021-12-14T15:16:16.048101Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_dataset.no_label_data.to_json(\"./labelled_nowiki.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
